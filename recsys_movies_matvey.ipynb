{
 "cells": [
  {
   "cell_type": "code",
   "id": "c0daec55",
   "metadata": {},
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Set, List, Tuple, Callable\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from scipy import sparse\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ed5c1996",
   "metadata": {},
   "source": [
    "# =======================\n",
    "# Section 1: Data Loading & Exploratory Data Analysis (EDA)\n",
    "# =======================\n",
    "\n",
    "BASE_PATH = os.path.join('2 кейс/ml-25m')\n",
    "\n",
    "\n",
    "def load_datasets(base_path: str) -> dict:\n",
    "    files = {\n",
    "        'movies': 'movies.csv',\n",
    "        'ratings': 'ratings.csv',\n",
    "        'tags': 'tags.csv',\n",
    "        'genome_tags': 'genome-tags.csv',\n",
    "        'genome_scores': 'genome-scores.csv',\n",
    "    }\n",
    "\n",
    "    dfs: dict[str, pd.DataFrame] = {}\n",
    "    for key, filename in files.items():\n",
    "        csv_path = os.path.join(base_path, filename)\n",
    "        print(f\"Loading {filename} …\")\n",
    "\n",
    "        # Apply light dtypes optimisation for large files\n",
    "        if key == 'ratings':\n",
    "            dtype = {\n",
    "                'userId': 'int32',\n",
    "                'movieId': 'int32',\n",
    "                'rating': 'float32',\n",
    "                'timestamp': 'int32',\n",
    "            }\n",
    "            df = pd.read_csv(csv_path, dtype=dtype)\n",
    "        elif key in {'movies', 'genome_tags'}:\n",
    "            df = pd.read_csv(csv_path)\n",
    "        else:\n",
    "            # tags & genome_scores\n",
    "            df = pd.read_csv(csv_path)\n",
    "\n",
    "        dfs[key] = df\n",
    "        print(f\"  → {key}: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def basic_eda(dfs: dict) -> None:\n",
    "    \"\"\"Run a minimal EDA and print key stats.\"\"\"\n",
    "\n",
    "    print(\"\\n========== Missing Values (per dataset) ==========\")\n",
    "    for name, df in dfs.items():\n",
    "        miss_total = int(df.isna().sum().sum())\n",
    "        print(f\"{name:<12}: {miss_total:,} missing values\")\n",
    "        if miss_total:\n",
    "            print(df.isna().sum())\n",
    "\n",
    "    # Ratings-specific statistics\n",
    "    ratings = dfs['ratings']\n",
    "    movies = dfs['movies']\n",
    "    tags = dfs['tags']\n",
    "\n",
    "    print(\"\\n========== Ratings Distribution ==========\")\n",
    "    print(ratings['rating'].describe())\n",
    "\n",
    "    print(\"\\n========== Unique Counts ==========\")\n",
    "    print(f\"Users          : {ratings['userId'].nunique():,}\")\n",
    "    print(f\"Rated movies   : {ratings['movieId'].nunique():,}\")\n",
    "    print(f\"Tagged movies  : {tags['movieId'].nunique():,}\")\n",
    "\n",
    "    print(\"\\n========== Top-10 Most Rated Movies ==========\")\n",
    "    top = (ratings.groupby('movieId')\n",
    "                  .size()\n",
    "                  .sort_values(ascending=False)\n",
    "                  .head(10)\n",
    "                  .reset_index(name='num_ratings'))\n",
    "    top = top.merge(movies[['movieId', 'title']], on='movieId', how='left')\n",
    "    print(top[['title', 'num_ratings']])\n",
    "\n",
    "    # Temporal coverage\n",
    "    ratings['datetime'] = pd.to_datetime(ratings['timestamp'], unit='s')\n",
    "    tags['datetime'] = pd.to_datetime(tags['timestamp'], unit='s')\n",
    "    print(\"\\n========== Temporal Coverage ==========\")\n",
    "    print(f\"Ratings : {ratings['datetime'].min()} → {ratings['datetime'].max()}\")\n",
    "    print(f\"Tags    : {tags['datetime'].min()} → {tags['datetime'].max()}\")\n",
    "\n",
    "\n",
    "\n",
    "datasets = load_datasets(BASE_PATH)\n",
    "basic_eda(datasets)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "34688f7e",
   "metadata": {},
   "source": [
    "def leave_one_out_split(ratings: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    ratings_sorted = ratings.sort_values(['userId', 'timestamp'])\n",
    "    # Mark the last interaction per user as test\n",
    "    idx = ratings_sorted.groupby('userId').tail(1).index\n",
    "    test_df = ratings_sorted.loc[idx]\n",
    "    train_df = ratings_sorted.drop(index=idx)  # type: ignore[arg-type]\n",
    "    print(f\"Train size: {train_df.shape[0]:,}, Test size: {test_df.shape[0]:,}\")\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "# --- Per-user metric helpers ---\n",
    "\n",
    "def precision_at_k(recommended: List[int], relevant: Set[int], k: int) -> float:\n",
    "    if k == 0:\n",
    "        return 0.0\n",
    "    hit_count = len(set(recommended[:k]) & relevant)\n",
    "    return hit_count / k\n",
    "\n",
    "\n",
    "def recall_at_k(recommended: List[int], relevant: Set[int], k: int) -> float:\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    hit_count = len(set(recommended[:k]) & relevant)\n",
    "    return hit_count / len(relevant)\n",
    "\n",
    "\n",
    "def apk(recommended: List[int], relevant: Set[int], k: int) -> float:\n",
    "    \"\"\"Average Precision at K\"\"\"\n",
    "    score = 0.0\n",
    "    hits = 0\n",
    "    for i, rec in enumerate(recommended[:k], start=1):\n",
    "        if rec in relevant:\n",
    "            hits += 1\n",
    "            score += hits / i\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    return score / min(len(relevant), k)\n",
    "\n",
    "\n",
    "def ndcg_at_k(recommended: List[int], relevant: Set[int], k: int) -> float:\n",
    "    dcg = 0.0\n",
    "    for i, rec in enumerate(recommended[:k], start=1):\n",
    "        if rec in relevant:\n",
    "            dcg += 1 / math.log2(i + 1)\n",
    "    ideal_hits = min(len(relevant), k)\n",
    "    idcg = sum(1 / math.log2(i + 1) for i in range(1, ideal_hits + 1))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "\n",
    "MetricFunc = Callable[[List[int], Set[int], int], float]\n",
    "\n",
    "\n",
    "METRIC_FUNCS: dict[str, MetricFunc] = {\n",
    "    'precision': precision_at_k,\n",
    "    'recall': recall_at_k,\n",
    "    'map': apk,\n",
    "    'ndcg': ndcg_at_k,\n",
    "}\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    recommend_fn: Callable[[int, int], List[int]],\n",
    "    test_user_movie: Dict[int, Set[int]],\n",
    "    k: int = 10,\n",
    "    metrics: Tuple[str, ...] = ('precision', 'recall', 'map', 'ndcg'),\n",
    ") -> dict:\n",
    "    \"\"\"Compute selected metrics for the provided recommender.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    recommend_fn : callable\n",
    "        Function (user_id, k) -> List[movieId].\n",
    "    test_user_movie : dict\n",
    "        Ground-truth sets for each user.\n",
    "    k : int\n",
    "        Cut-off rank.\n",
    "    metrics : tuple[str]\n",
    "        Metric names to compute.\n",
    "    \"\"\"\n",
    "    results = {m: [] for m in metrics}\n",
    "    for uid, relevant in test_user_movie.items():\n",
    "        recs = recommend_fn(uid, k)\n",
    "        for m in metrics:\n",
    "            func = METRIC_FUNCS[m]\n",
    "            results[m].append(func(recs, relevant, k))\n",
    "\n",
    "    aggregated = {m: float(pd.Series(vals).mean()) for m, vals in results.items()}\n",
    "    return aggregated"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =======================\n",
    "# Section 2: Pre-processing Helpers\n",
    "# =======================\n",
    "\n",
    "\n",
    "def build_user_movie_sets(ratings: pd.DataFrame, min_ratings: int = 5) -> Dict[int, Set[int]]:\n",
    "    user_movie: Dict[int, Set[int]] = defaultdict(set)\n",
    "    for row in ratings.itertuples(index=False):\n",
    "        user_movie[row.userId].add(row.movieId)  # type: ignore[attr-defined]\n",
    "\n",
    "    # Filter sparse users\n",
    "    if min_ratings > 0:\n",
    "        user_movie = {u: movies for u, movies in user_movie.items() if len(movies) >= min_ratings}\n",
    "    print(f\"Kept {len(user_movie):,} users with ≥{min_ratings} ratings each (from {ratings['userId'].nunique():,} total)\")\n",
    "    return user_movie"
   ],
   "id": "6ebe64ecc86d1973"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Part 2.1 - Matrix Factorization with ALS\n",
    "\n",
    "Alternating Least Squares for Implicit Feedback\n",
    "Core Idea\n",
    "We implement weighted matrix factorization optimized for implicit feedback data (e.g., movie ratings treated as confidence values). The method:\\\n",
    "- Decomposes user-item interactions into low-dimensional latent factors\n",
    "- Uses alternating optimization with regularization to prevent overfitting\n",
    "- Scales linearly with the number of factors (unlike exact SVD)\n",
    "- Handles sparse data efficiently via conjugate gradient method\n",
    "\n",
    "Input Data Transformation\\\n",
    "Convert explicit ratings to implicit confidence weights:\n",
    "$$ c_{ui} = 1 + \\alpha ⋅ r_{ui} $$\n",
    "\n",
    "where $\\alpha$ controls how strongly high ratings should be weighted\n",
    "\n",
    "Matrix Factorization Model\n",
    "Factorize the interaction matrix $R$ into:\n",
    "- User factors: $X \\in \\mathbb{R}^{|U|\\times k}$\n",
    "- Item factors: $Y \\in \\mathbb{R}^{|M|\\times k}$\n",
    "\n",
    "Objective function with L2 regularization:\\\n",
    "$$ \\min_{X,Y} \\sum_{u,i} c_{ui}(p_{ui} - \\mathbf{x}_u^T \\mathbf{y}_i)^2 + \\lambda\\left(\\|X\\|_F^2 + \\|Y\\|_F^2\\right) $$\n",
    "\n",
    "where $p_{ui} = 1$ if user $u$ interacted with item $i$, else 0\n",
    "\n",
    "ALS Optimization\\\n",
    "Alternating between:\n",
    "\n",
    "1) Fix $Y$, solve for $X$:\\\n",
    "$$ x_u = (Y^T C_u Y + \\lambda I)^{-1} Y^T C_u p_u $$\n",
    "2) Fix $X$, solve for $Y$:\\\n",
    "$$ y_i = (X^T C_u X + \\lambda I)^{-1} X^T C_i p_i $$\n",
    "\n",
    "\n",
    "Implementation Details\n",
    "- Factors: $k=64$ (tunable latent dimension)\n",
    "- Regularization: $\\lambda=0.05$ (controls overfitting)\n",
    "- Iterations: 15 (trade-off between convergence and speed)\n",
    "- Alpha: 40.0 (controls how strongly to weight observed ratings)\n",
    "\n",
    "Recommendation Generation\\\n",
    "For user $u$:\n",
    "\n",
    "1) Compute user's latent vector $x_u$\n",
    "2) Score all items:\n",
    "3) Filter out seen items (optional)\n",
    "4) Return top-$k$ items by predicted score\n",
    "\n",
    "Computational Complexity\\\n",
    "Per iteration:\n",
    "\n",
    "- $O(k^2|U| + k^3|M|)$ for user updates\n",
    "- $O(k^2|M| + k^3|U|)$ for item updates\\\n",
    "Linear in number of users/items when $k$ is fixed\n",
    "\n",
    "Advantages over Neighborhood Methods\n",
    "- Better cold-start handling via shared factors\n",
    "- Captures transitive relationships (A likes B, B likes C → A might like C)\n",
    "- More compact representation ($k$ factors per user/item vs. full vectors)"
   ],
   "id": "e3cc7b3e3b98cd30"
  },
  {
   "cell_type": "code",
   "id": "bf1ec6a6",
   "metadata": {},
   "source": [
    "# =======================\n",
    "# Section 2.1: Collaborative Filtering via MinHash-LSH\n",
    "# =======================\n",
    "\n",
    "\n",
    "class ALSRecommender:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        factors: int = 64,\n",
    "        regularization: float = 0.05,\n",
    "        iterations: int = 15,\n",
    "        alpha: float = 40.0,\n",
    "        random_state: int = 42,\n",
    "    ) -> None:\n",
    "        self.factors = factors\n",
    "        self.regularization = regularization\n",
    "        self.iterations = iterations\n",
    "        self.alpha = alpha\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.model: AlternatingLeastSquares | None = None\n",
    "        self.user2idx: dict[int, int] = {}\n",
    "        self.idx2user: dict[int, int] = {}\n",
    "        self.item2idx: dict[int, int] = {}\n",
    "        self.idx2item: dict[int, int] = {}\n",
    "        self.user_items: sparse.csr_matrix | None = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_implicit_weight(rating: float, alpha: float) -> float:\n",
    "        return 1.0 + alpha * rating\n",
    "\n",
    "    def _build_mappings(self, ratings: pd.DataFrame) -> None:\n",
    "        users = ratings[\"userId\"].unique()\n",
    "        items = ratings[\"movieId\"].unique()\n",
    "\n",
    "        self.user2idx = {u: i for i, u in enumerate(users)}\n",
    "        self.idx2user = {i: u for u, i in self.user2idx.items()}\n",
    "        self.item2idx = {m: i for i, m in enumerate(items)}\n",
    "        self.idx2item = {i: m for m, i in self.item2idx.items()}\n",
    "\n",
    "    def _build_matrix(self, ratings: pd.DataFrame) -> sparse.csr_matrix:\n",
    "        row = ratings[\"userId\"].map(self.user2idx).to_numpy()\n",
    "        col = ratings[\"movieId\"].map(self.item2idx).to_numpy()\n",
    "        data = self._to_implicit_weight(ratings[\"rating\"].astype(float), self.alpha)\n",
    "\n",
    "        mat = sparse.coo_matrix(\n",
    "            (data, (row, col)),\n",
    "            shape=(len(self.user2idx), len(self.item2idx)),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        return mat.tocsr()\n",
    "\n",
    "    def fit(self, ratings_df: pd.DataFrame) -> None:\n",
    "        # 1. Copy Id → index\n",
    "        self._build_mappings(ratings_df)\n",
    "\n",
    "        # 2. make a matrix\n",
    "        self.user_items = self._build_matrix(ratings_df)\n",
    "\n",
    "        # 3. train ALS\n",
    "        self.model = AlternatingLeastSquares(\n",
    "            factors=self.factors,\n",
    "            regularization=self.regularization,\n",
    "            iterations=self.iterations,\n",
    "            random_state=self.random_state,\n",
    "        )\n",
    "        # implicit awaits (items × users) — transpond\n",
    "        self.model.fit(self.user_items.T)\n",
    "\n",
    "    def recommend(\n",
    "        self,\n",
    "        uid: int,\n",
    "        seen: set[int] | None = None,\n",
    "        top_k: int = 10,\n",
    "    ) -> list[int]:\n",
    "        if self.model is None or self.user_items is None:\n",
    "            raise RuntimeError(\"Model not trained yet.\")\n",
    "        if uid not in self.user2idx:\n",
    "            return []\n",
    "\n",
    "        seen = seen or set()\n",
    "        uidx = self.user2idx[uid]\n",
    "\n",
    "        # ① берём ровно одну строку (1 × num_items) в формате CSR\n",
    "        user_row = self.user_items[uidx]\n",
    "\n",
    "        # ② получаем рекомендации\n",
    "        recs, _ = self.model.recommend(\n",
    "            uidx,                 # userid позиционно\n",
    "            user_row,             # одна строка\n",
    "            N=top_k + len(seen),\n",
    "            filter_items=[self.item2idx.get(m, m) for m in seen],  # защитимся тем же способом\n",
    "        )\n",
    "\n",
    "        # ③ переводим в movieId, учитывая оба возможных случая\n",
    "        rec_movie_ids: list[int] = []\n",
    "        for i in recs:\n",
    "            mid = self.idx2item.get(int(i), int(i))  # если нет — значит i уже movieId\n",
    "            if mid not in seen:\n",
    "                rec_movie_ids.append(mid)\n",
    "\n",
    "        return rec_movie_ids[:top_k]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Part 2.2 - Content filtration\n",
    "\n",
    "#### Core idea\n",
    "Let $e_i \\in \\mathbb{R}^n$ be a content embedding vector for item $i$. We can estimate dot product or cosine distance before user rated it and rate in by formula:\n",
    "$$\n",
    "\\hat{r}_{ui} = \\max_{j \\in I_u, \\, r_{uj} > \\alpha} \\rho(e_i, e_j) \\, r_{uj},\n",
    "$$\n",
    "where $\\rho$ - dot product or cosine distance between two vectors, $I_u$ - set of rated movies by user, and $\\alpha$ - a hyperparameter.\\\n",
    "(just a simple ranking model)"
   ],
   "id": "1804ad06c8d40cfb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =======================\n",
    "# Section 2.2: Simple Genre-Based Content Filtering\n",
    "# =======================\n",
    "\n",
    "\n",
    "def parse_movie_genres(movies: pd.DataFrame) -> Dict[int, Set[str]]:\n",
    "    genre_map: Dict[int, Set[str]] = {}\n",
    "    for row in movies.itertuples(index=False):\n",
    "        genres = set() if row.genres == \"(no genres listed)\" else set(row.genres.split(\"|\"))  # type: ignore[attr-defined]\n",
    "        genre_map[row.movieId] = genres  # type: ignore[attr-defined]\n",
    "    return genre_map\n",
    "\n",
    "\n",
    "def content_recommend(uid: int, user_movie: Dict[int, Set[int]], movie_genres: Dict[int, Set[str]], top_k: int = 10) -> List[int]:\n",
    "    seen = user_movie.get(uid, set())\n",
    "    if not seen:\n",
    "        return []\n",
    "\n",
    "    # Build user genre profile\n",
    "    genre_counter: Dict[str, int] = defaultdict(int)\n",
    "    for m in seen:\n",
    "        for g in movie_genres.get(m, set()):\n",
    "            genre_counter[g] += 1\n",
    "\n",
    "    if not genre_counter:\n",
    "        return []\n",
    "\n",
    "    # Score candidate movies by summed genre counts\n",
    "    candidate_scores: Dict[int, int] = {}\n",
    "    for movie_id, genres in movie_genres.items():\n",
    "        if movie_id in seen:\n",
    "            continue\n",
    "        score = sum(genre_counter[g] for g in genres)\n",
    "        if score:\n",
    "            candidate_scores[movie_id] = score\n",
    "\n",
    "    ranked = sorted(candidate_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [m for m, _ in ranked[:top_k]]"
   ],
   "id": "3c6a1f719bc859be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Part 2.3 - Hybrid approach (DSSM)\n",
    "\n",
    "Both described approaches are good but have some problems.\\\n",
    "Biggest problem for a recommendation model is a filter bubble. In case we use collaborative approach we don't have any information about new users so we can't recommend them something. In case we use content filtration we have a bigger filter bubble so we should use more complicated model to solve that problem.\n",
    "\n",
    "In this solution we will use a `DSSM` model:\\\n",
    "`DSSM` learns a nonlinear mapping between two discrete entities (typically queries and documents) into a shared latent semantic space where similarity can be computed via cosine distance.\n",
    "\n",
    "Given:\\\n",
    "Query $q$ and document $d$\\\n",
    "Their embeddings $y(q)$ and $y(d)$ in $\\mathbb{R}^k$\\\n",
    "Goal: Maximize similarity for relevant $R(q,d)$ pairs: $$ \\text{sim}(y_q, y_d) = \\text{cos}(y(q), y(d)) = \\frac{y_{q}^{T}y_d}{||y_q||⋅||y_d||} $$\n",
    "\n",
    "For this task we will choose cross-entropy with negative sampling\n",
    "\n",
    "Cross-entropy formula: $$ \\mathcal{L}(q, d^+) = -\\log \\big( P(d^+ \\mid q) \\big) $$\n",
    "\n",
    "The problem is in gradient computing complexity for $ \\mathcal{L}(q, d^+) $ because we should calculate click prbability of every movie by every query. So the solution is in negative sampling. Note that among the documents $d$ in the denominator of $P(d∣q)$, only one is typically clicked (positive example), while the remaining thousands or millions serve as negative examples. Rather than computing the full summation over all documents at each optimization step, it is computationally efficient to consider only a small sampled subset.\\\n",
    "So our result function is: $$ \\exp \\big( b_0 R(q, d^{+}) \\big) + \\sum_{i=1}^{k} \\exp \\big( b_0 R(q, d_i^{-}) \\big), $$ where $d_{1}^{-},...,d_{k}^{-} $ - negative samples for for $q$ query.\\\n",
    "For generating this samples we will equally likely select a subset of movies from the unrated ones. In the original article about DSSM, a recommented ratio is 4:1."
   ],
   "id": "fda8c8f24734a42f"
  },
  {
   "cell_type": "code",
   "id": "316d4ffa",
   "metadata": {},
   "source": [
    "# =======================\n",
    "# Section 2.3: Hybrid DSSM Model (PyTorch)\n",
    "# =======================\n",
    "\n",
    "\n",
    "class _PairDataset(Dataset):\n",
    "    \"\"\"Dataset yielding (user_idx, pos_item_idx, neg_item_idxs).\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        user_pos: Dict[int, Set[int]],\n",
    "        num_items: int,\n",
    "        neg_ratio: int = 4,\n",
    "    ) -> None:\n",
    "        self.user_indices = list(user_pos.keys())\n",
    "        self.user_pos = user_pos\n",
    "        self.num_items = num_items\n",
    "        self.neg_ratio = neg_ratio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        uid = self.user_indices[idx]\n",
    "        pos_items = list(self.user_pos[uid])\n",
    "        pos_item = random.choice(pos_items)\n",
    "        neg_items = []\n",
    "        # sample negatives distinct from positives\n",
    "        while len(neg_items) < self.neg_ratio:\n",
    "            neg = random.randint(0, self.num_items - 1)\n",
    "            if neg not in self.user_pos[uid]:\n",
    "                neg_items.append(neg)\n",
    "        return uid, pos_item, torch.tensor(neg_items, dtype=torch.long)  # type: ignore[name-defined]\n",
    "\n",
    "\n",
    "class DSSMRecommender:\n",
    "    \"\"\"Hybrid DSSM with separate user & item towers.\n",
    "\n",
    "    The simplest variant: both towers are embedding layers optionally followed by MLP.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int = 64,\n",
    "        hidden_dims: Tuple[int, ...] = (),  # type: ignore[valid-type]\n",
    "        neg_ratio: int = 4,\n",
    "        epochs: int = 5,\n",
    "        batch_size: int = 1024,\n",
    "        lr: float = 1e-3,\n",
    "        rating_threshold: float = 3.5,\n",
    "        device: str | None = None,\n",
    "    ) -> None:\n",
    "        if torch is None:\n",
    "            raise ImportError(\"PyTorch is required for DSSM model. Install via `pip install torch`. \")\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.neg_ratio = neg_ratio\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.rating_threshold = rating_threshold\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Will be initialised in fit\n",
    "        self.user_embedding: nn.Module | None = None  # type: ignore[valid-type]\n",
    "        self.item_embedding: nn.Module | None = None  # type: ignore[valid-type]\n",
    "        self.user_mlp: nn.Module | None = None  # type: ignore[valid-type]\n",
    "        self.item_mlp: nn.Module | None = None  # type: ignore[valid-type]\n",
    "        self.user2idx: Dict[int, int] = {}\n",
    "        self.idx2item: Dict[int, int] = {}\n",
    "        self.item_vecs: torch.Tensor | None = None  # cached item representations\n",
    "\n",
    "    # ----------------- Utility builders -----------------\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_mlp(input_dim: int, hidden_dims: Tuple[int, ...]) -> nn.Module:  # type: ignore[name-defined]\n",
    "        layers: list[nn.Module] = []\n",
    "        prev_dim = input_dim\n",
    "        for dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, dim))  # type: ignore[attr-defined]\n",
    "            layers.append(nn.ReLU())  # type: ignore[attr-defined]\n",
    "            prev_dim = dim\n",
    "        return nn.Sequential(*layers)  # type: ignore[attr-defined]\n",
    "\n",
    "    # ----------------- Training -----------------\n",
    "\n",
    "    def fit(self, ratings: pd.DataFrame):\n",
    "        \"\"\"Train DSSM on implicit feedback derived from *ratings* DataFrame.\"\"\"\n",
    "        import numpy as np  # noqa: F401\n",
    "        import random  # noqa: F401\n",
    "\n",
    "        # Map ids to contiguous indices\n",
    "        unique_users = ratings['userId'].unique()\n",
    "        unique_items = ratings['movieId'].unique()\n",
    "        self.user2idx = {uid: i for i, uid in enumerate(unique_users)}\n",
    "        item2idx = {mid: i for i, mid in enumerate(unique_items)}\n",
    "        self.idx2item = {i: mid for mid, i in item2idx.items()}\n",
    "\n",
    "        num_users = len(unique_users)\n",
    "        num_items = len(unique_items)\n",
    "\n",
    "        # Build implicit feedback sets\n",
    "        user_pos: Dict[int, Set[int]] = defaultdict(set)\n",
    "        for row in ratings.itertuples(index=False):\n",
    "            if row.rating >= self.rating_threshold:  # type: ignore[attr-defined]\n",
    "                u_idx = self.user2idx[row.userId]  # type: ignore[attr-defined]\n",
    "                i_idx = item2idx[row.movieId]  # type: ignore[attr-defined]\n",
    "                user_pos[u_idx].add(i_idx)\n",
    "\n",
    "        # Filter users with no positives\n",
    "        user_pos = {u: items for u, items in user_pos.items() if items}\n",
    "\n",
    "        dataset = _PairDataset(user_pos, num_items, neg_ratio=self.neg_ratio)\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "        # Build model components\n",
    "        self.user_embedding = nn.Embedding(num_users, self.embedding_dim).to(self.device)  # type: ignore[attr-defined]\n",
    "        self.item_embedding = nn.Embedding(num_items, self.embedding_dim).to(self.device)  # type: ignore[attr-defined]\n",
    "        self.user_mlp = self._build_mlp(self.embedding_dim, self.hidden_dims).to(self.device)\n",
    "        self.item_mlp = self._build_mlp(self.embedding_dim, self.hidden_dims).to(self.device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(  # type: ignore[attr-defined]\n",
    "            list(self.user_embedding.parameters())  # type: ignore[union-attr]\n",
    "            + list(self.item_embedding.parameters())  # type: ignore[union-attr]\n",
    "            + list(self.user_mlp.parameters())  # type: ignore[union-attr]\n",
    "            + list(self.item_mlp.parameters()),  # type: ignore[union-attr]\n",
    "            lr=self.lr,\n",
    "        )\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            total_loss = 0.0\n",
    "            for u_idxs, pos_i_idxs, neg_i_idxs in loader:\n",
    "                u_idxs = u_idxs.to(self.device)\n",
    "                pos_i_idxs = pos_i_idxs.to(self.device)\n",
    "                neg_i_idxs = neg_i_idxs.to(self.device)\n",
    "\n",
    "                # Forward pass\n",
    "                user_vec = self.user_mlp(self.user_embedding(u_idxs))  # (B, d)\n",
    "                pos_vec = self.item_mlp(self.item_embedding(pos_i_idxs))  # (B, d)\n",
    "                neg_vec = self.item_mlp(self.item_embedding(neg_i_idxs))  # (B, neg, d)\n",
    "\n",
    "                # Normalize\n",
    "                user_vec = nn.functional.normalize(user_vec, dim=1)\n",
    "                pos_vec = nn.functional.normalize(pos_vec, dim=1)\n",
    "                neg_vec = nn.functional.normalize(neg_vec, dim=2)\n",
    "\n",
    "                # Positive scores (B, 1)\n",
    "                pos_scores = (user_vec * pos_vec).sum(dim=1, keepdim=True)\n",
    "                # Negative scores (B, neg)\n",
    "                neg_scores = torch.bmm(neg_vec, user_vec.unsqueeze(2)).squeeze(2)  # (B, neg)\n",
    "\n",
    "                logits = torch.cat([pos_scores, neg_scores], dim=1)  # (B, 1+neg)\n",
    "                labels = torch.zeros(logits.size(0), dtype=torch.long, device=self.device)\n",
    "                loss = nn.functional.cross_entropy(logits, labels)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_loss = total_loss / len(loader)\n",
    "            print(f\"Epoch {epoch}/{self.epochs} - loss {avg_loss:.4f}\")\n",
    "\n",
    "        # Cache item vectors for fast inference\n",
    "        with torch.no_grad():\n",
    "            all_item_idx = torch.arange(num_items, device=self.device)\n",
    "            item_vecs = self.item_mlp(self.item_embedding(all_item_idx))\n",
    "            self.item_vecs = nn.functional.normalize(item_vecs, dim=1).cpu()\n",
    "\n",
    "    # ----------------- Recommendation -----------------\n",
    "\n",
    "    def _get_user_vec(self, uid: int) -> torch.Tensor:\n",
    "        if self.user_embedding is None:\n",
    "            raise RuntimeError(\"Model not trained.\")\n",
    "        if uid not in self.user2idx:\n",
    "            raise ValueError(\"Unknown user id\")\n",
    "        self.user_embedding.eval()\n",
    "        with torch.no_grad():\n",
    "            idx = torch.tensor([self.user2idx[uid]], device=self.device)\n",
    "            vec = self.user_mlp(self.user_embedding(idx))\n",
    "            return nn.functional.normalize(vec, dim=1).cpu().squeeze(0)\n",
    "\n",
    "    def recommend(self, uid: int, seen: Set[int], top_k: int = 10) -> List[int]:\n",
    "        \"\"\"Return top_k recommended movieIds not in *seen*.\"\"\"\n",
    "        import numpy as np\n",
    "        if self.item_vecs is None:\n",
    "            raise RuntimeError(\"Model not trained.\")\n",
    "        user_vec = self._get_user_vec(uid)\n",
    "        scores = torch.mv(self.item_vecs, user_vec)  # (num_items,)\n",
    "        scores_numpy = scores.numpy()\n",
    "        # Mask seen\n",
    "        for mid in seen:\n",
    "            if mid in self.idx2item.values():\n",
    "                idx = list(self.idx2item.keys())[list(self.idx2item.values()).index(mid)]\n",
    "                scores_numpy[idx] = -np.inf\n",
    "        top_indices = scores_numpy.argsort()[-top_k:][::-1]\n",
    "        return [self.idx2item[i] for i in top_indices]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a60db736",
   "metadata": {},
   "source": [
    "# leave-one-out split\n",
    "train_df, test_df = leave_one_out_split(datasets['ratings'])\n",
    "\n",
    "# sets of rated films\n",
    "train_user_movies = build_user_movie_sets(train_df)\n",
    "test_user_movies  = build_user_movie_sets(test_df, min_ratings=1)   # ≥1, иначе всех потеряем\n",
    "\n",
    "# genres for content filtering\n",
    "movie_genres_map = parse_movie_genres(datasets['movies'])\n",
    "\n",
    "# =======================\n",
    "# 1. Collaborative: ALS\n",
    "# =======================\n",
    "als_rec = ALSRecommender(factors=100, iterations=20, alpha=40)\n",
    "als_rec.fit(train_df)\n",
    "\n",
    "als_results = evaluate_model(\n",
    "    recommend_fn=lambda u, k: als_rec.recommend(\n",
    "        u, seen=train_user_movies.get(u, set()), top_k=k\n",
    "    ),\n",
    "    test_user_movie=test_user_movies,\n",
    "    k=10,\n",
    "    metrics=('precision', 'recall', 'map', 'ndcg'),\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# 2. Content: genres\n",
    "# =======================\n",
    "content_results = evaluate_model(\n",
    "    recommend_fn=lambda u, k: content_recommend(\n",
    "        u, user_movie=train_user_movies, movie_genres=movie_genres_map, top_k=k\n",
    "    ),\n",
    "    test_user_movie=test_user_movies,\n",
    "    k=10,\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# 3. Hybrid: DSSM\n",
    "# =======================\n",
    "dssm = DSSMRecommender(embedding_dim=64, epochs=3)\n",
    "dssm.fit(train_df)\n",
    "\n",
    "dssm_results = evaluate_model(\n",
    "    recommend_fn=lambda u, k: dssm.recommend(\n",
    "        u, seen=train_user_movies.get(u, set()), top_k=k\n",
    "    ),\n",
    "    test_user_movie=test_user_movies,\n",
    "    k=10,\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# 4. Metrics\n",
    "# =======================\n",
    "results_df = pd.DataFrame(\n",
    "    [als_results, content_results, dssm_results],\n",
    "    index=['ALS (collab)', 'Content (genres)', 'DSSM (hybrid)'],\n",
    ").round(4)\n",
    "\n",
    "display(results_df)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
